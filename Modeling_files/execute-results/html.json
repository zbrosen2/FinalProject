{
  "hash": "b0f5763e9ff7fb5d8cd2ef0c7156d92f",
  "result": {
    "markdown": "---\ntitle: \"Modeling\"\nformat: html\neditor: visual\n---\n\n## Author and Date\nAuthor: Zachary Rosen\n\nDate: 7/29/2025\n\n## Introduction\nIn this project I'll be working with the Diabetes Health Indicators Dataset which can be found at www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/.  Specifically, I will be looking at the diabetes_binary_health_indicators_BRFSS2015.csv data.  There are many variables in this dataset, but for the purposes of this project, I will be limiting the scope to the following variables: Diabetes_binary, HighBP, HighChol, Smoker, PhysActivity, Age, and BMI.  Each of these variables is categorical except for BMI.  Diabetes_binary is the response variable, where 0 represents no diabetes and 1 represents prediabetes or diabetes.  The rest of the variables are predictors of Diabetes_binary, where HighBP at level 0 indicates no high blood pressure and 1 indicates high blood pressure, HighChol at level 0 indicates no high cholesterol and 1 indicates high cholesterol, Smoker at level 0 indicates that the person hasn't smoked at least 100 cigarettes in their life and 1 indicates that they have, and PhysActivity at level 0 indicates no physical activity in the last 30 days (not including a job) and 1 indicates physical activity.  Age is a 13 level categorical variable, from younger to older age groups in ascending order.  Lastly, BMI is a numeric variable that represents the body mass index.\n\nThe purpose of this modeling file is to develop and compare predictive models for diabetes status using key predictors.  By evaluating logistic regression models, classification trees, and random forests with cross-validated log loss, I aim to identify the best performing model to accurately predict diabetes status.  First I will identify the best performing model in each category using cross-validation, and then I will fit a final model in each category using the whole training set.  Finally, I will evaluate each of the three \"best\" models on the testing set and declare a winner.\n\n## Data\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(caret)\nlibrary(ranger)\nlibrary(yardstick)\n\ndiabetes <- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# convert categorical variables to factors\ndiabetes <- diabetes |>\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"N\", \"Y\")),\n         HighBP = factor(HighBP),\n         HighChol = factor(HighChol),\n         Smoker = factor(Smoker),\n         PhysActivity = factor(PhysActivity),\n         Age = factor(Age, ordered = TRUE)\n         )\n\n# split data into train/test (70/30)\ndata_split <- initial_split(diabetes, prop = 0.7)\ntrain <- training(data_split)\ntest <- testing(data_split)\n\n# 5-fold cv using log loss\ncontrol <- trainControl(method = \"cv\", number = 5, classProbs = TRUE,\n                            summaryFunction = mnLogLoss)\n```\n:::\n\n\n## Logistic Regression Models\nLogistic regression can be used as a classification model when the response variable is binary.  It estimates the probability that an observation belongs to a particular class (e.g. diabetic or not).  It models the log-odds of the outcome as a linear combination of the predictors.  Logistic regression is useful here since I'm working with a binary outcome (Diabetes_binary) and want to understand how different predictors are associated with diabetes risk.  It provides interpretable coefficients and works well with log loss as a metric.  This is because log loss directly measures the accuracy of predicted probabilities (and logistic regression outputs these probabilities).\n\n::: {.cell}\n\n```{.r .cell-code}\n# set tuning grid for alpha (0 is ridge regression, 1 is LASSO and 0.5 is elastic net)\n# lambda is the penalty hyperparameter\ngrid_LR <- expand.grid(\n  alpha = c(0, 0.5, 1),\n  lambda = exp(seq(log(0.001), log(10), length.out = 200))\n)\n\n# simple additive model\nLR_1 <- train(Diabetes_binary ~ PhysActivity + Age + BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# similar to LR_1 but with an added interaction term between Age and BMI\nLR_2 <- train(Diabetes_binary ~ PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# similar to LR_2 but with HighBP, HighChol, and Smoker as additional predictors\nLR_3 <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                + PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# get training performances for each model\ngetTrainPerf(LR_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  TrainlogLoss method\n1    0.3578917 glmnet\n```\n:::\n\n```{.r .cell-code}\ngetTrainPerf(LR_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  TrainlogLoss method\n1    0.3575414 glmnet\n```\n:::\n\n```{.r .cell-code}\ngetTrainPerf(LR_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  TrainlogLoss method\n1     0.338274 glmnet\n```\n:::\n\n```{.r .cell-code}\n# LR_3 is winner (lowest log loss)\n# fit final model without cv and with best values for hyperparameters\nLR_3_final <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                + PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = trainControl(method = \"none\", classProbs = TRUE),\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = LR_3$bestTune\n)\n\n# predict using test data\nLR_3_probs <- predict(LR_3_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_LR_3 <- mn_log_loss(\n  data = bind_cols(test, LR_3_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nLR_3$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    alpha lambda\n201   0.5  0.001\n```\n:::\n:::\n\n\nLR_2 has slightly lower log loss than LR_1 and LR_3 has slightly lower log loss than LR_2.  This indicates that adding an interaction term between Age and BMI did in fact help predictive performance while training.  Similarly, adding HighBP, HighChol, and Smoker as additional predictors also helped predictive performance while training (by quite a lot).  The model with the lowest log loss is chosen as the best model (LR_3).\n\nA classification tree is a decision tree that recursively splits the data based on predictor values to classify observations.  At each node, it chooses the best split to maximize class purity (using a metric like Gini impurity).  The final model is a tree where each leaf represents a class label.  These models are easy to interpret and can handle numeric and categorical predictors.  We might use this as they are especially helpful when the relationship between predictors and the outcome is complex.\n\n## Classification Tree\n\n::: {.cell}\n\n```{.r .cell-code}\n# set tuning grid for complexity parameter\ngrid_class_tree <- expand.grid(\n  cp = seq(0.0001, 0.01, length.out = 100)\n)\n\n# use model formula from LR_3 since it performed the best\n# but instead of logistic regression, use classification tree\nclass_tree <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                    data = train,\n                    metric = \"logLoss\",\n                    trControl = control,\n                    method = \"rpart\",\n                    tuneGrid = grid_class_tree\n)\n\n# get training performance\ngetTrainPerf(class_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  TrainlogLoss method\n1    0.3571625  rpart\n```\n:::\n\n```{.r .cell-code}\n# fit final model without cv and with best value for hyperparameter\nclass_tree_final <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                    data = train,\n                    metric = \"logLoss\",\n                    trControl = trainControl(method = \"none\", classProbs = TRUE),\n                    method = \"rpart\",\n                    tuneGrid = class_tree$bestTune\n)\n\n# predict using test data\nclass_tree_probs <- predict(class_tree_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_class_tree <- mn_log_loss(\n  data = bind_cols(test, class_tree_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nclass_tree$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     cp\n1 1e-04\n```\n:::\n:::\n\n\nThe tuned value of the complexity parameter is 0.0003, which is quite small.  This indicates that the model created a larger tree with many splits.  Overall the final model is relatively complex, which helps fit the training data well, but increases the risk of overfitting.\n\n## Random Forest\n\n::: {.cell}\n\n```{.r .cell-code}\n# set tuning grid for mtry (# of randomly selected predictors), splitrule (metric \n# used to determine splitting rule), and min.node.size (minimal node size)\ngrid_rand_forest <- expand.grid(\n  mtry = c(2, 3, 4, 5),\n  splitrule = c(\"gini\"),\n  min.node.size = c(1, 5, 10)\n)\n\n# same model formula as before but using a random forest model\nrand_forest <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                     data = train,\n                     metric = \"logLoss\",\n                     trControl = control,\n                     method = \"ranger\",\n                     tuneGrid = grid_rand_forest\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGrowing trees.. Progress: 99%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 65%. Estimated remaining time: 16 seconds.\nGrowing trees.. Progress: 100%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 62%. Estimated remaining time: 18 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 63%. Estimated remaining time: 18 seconds.\nGrowing trees.. Progress: 95%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 82%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 85%. Estimated remaining time: 5 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 90%. Estimated remaining time: 3 seconds.\nGrowing trees.. Progress: 75%. Estimated remaining time: 10 seconds.\nGrowing trees.. Progress: 84%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 76%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 87%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 81%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 98%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 83%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 66%. Estimated remaining time: 15 seconds.\nGrowing trees.. Progress: 80%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 65%. Estimated remaining time: 16 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 12 seconds.\nGrowing trees.. Progress: 92%. Estimated remaining time: 2 seconds.\nGrowing trees.. Progress: 86%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 88%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 91%. Estimated remaining time: 3 seconds.\nGrowing trees.. Progress: 69%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 86%. Estimated remaining time: 5 seconds.\n```\n:::\n\n```{.r .cell-code}\n# get training performance\ngetTrainPerf(rand_forest)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  TrainlogLoss method\n1    0.3380356 ranger\n```\n:::\n\n```{.r .cell-code}\n# fit final model without cv and with best values for hyperparameters\nrand_forest_final <- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                     data = train,\n                     metric = \"logLoss\",\n                     trControl = trainControl(method = \"none\", classProbs = TRUE),\n                     method = \"ranger\",\n                     tuneGrid = rand_forest$bestTune\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGrowing trees.. Progress: 87%. Estimated remaining time: 4 seconds.\n```\n:::\n\n```{.r .cell-code}\n# predict using test data\nrand_forest_probs <- predict(rand_forest_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_rand_forest <- mn_log_loss(\n  data = bind_cols(test, rand_forest_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nrand_forest$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  mtry splitrule min.node.size\n6    3      gini            10\n```\n:::\n:::\n\n\nThe tuned value of mtry was 3 and min.node.size was 10.  This likely helps balance model complexity and overfitting risk.\n\nA random forest is an ensemble method that builds on many decision trees (e.g. classification trees) on bootstrapped samples of the data and averages their predictions.  At each split, only a random subset of predictors is considered, which reduces overfitting.  It is often more stable than a traditional classification tree, and is often more accurate and less sensitive to small changes in the data.  In our case, it is useful for capturing complex patterns in the data without risking overfitting (like with a traditional classification tree).\n\n## Final Model Selection\n\n::: {.cell}\n\n```{.r .cell-code}\n# make tibble with \"best\" LR, class tree, and random forest models for comparison\ntibble(\n  model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  log_loss = c(log_loss_LR_3$.estimate, log_loss_class_tree$.estimate, \n               log_loss_rand_forest$.estimate)\n) |>\n  mutate(log_loss = format(log_loss, digits = 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  model               log_loss\n  <chr>               <chr>   \n1 Logistic Regression 0.34001 \n2 Classification Tree 0.35953 \n3 Random Forest       0.34013 \n```\n:::\n:::\n\n\nThe final \"best\" model is the logistic regression model, which just barely outperforms the random forest model (log loss of 0.34001 vs. 0.34014).  These are both significantly better performing than the classification tree (log loss of 0.35930).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}