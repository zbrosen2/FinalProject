[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Author: Zachary Rosen\nDate: 7/29/2025"
  },
  {
    "objectID": "EDA.html#author-and-date",
    "href": "EDA.html#author-and-date",
    "title": "EDA",
    "section": "",
    "text": "Author: Zachary Rosen\nDate: 7/29/2025"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\nIn this project I’ll be working with the Diabetes Health Indicators Dataset which can be found at www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/. Specifically, I will be looking at the diabetes_binary_health_indicators_BRFSS2015.csv data. There are many variables in this dataset, but for the purposes of this project, I will be limiting the scope to the following variables: Diabetes_binary, HighBP, HighChol, Smoker, PhysActivity, Age, and BMI. Each of these variables is categorical except for BMI. Diabetes_binary is the response variable, where 0 represents no diabetes and 1 represents prediabetes or diabetes. The rest of the variables are predictors of Diabetes_binary, where HighBP at level 0 indicates no high blood pressure and 1 indicates high blood pressure, HighChol at level 0 indicates no high cholesterol and 1 indicates high cholesterol, Smoker at level 0 indicates that the person hasn’t smoked at least 100 cigarettes in their life and 1 indicates that they have, and PhysActivity at level 0 indicates no physical activity in the last 30 days (not including a job) and 1 indicates physical activity. Age is a 13 level categorical variable, from younger to older age groups in ascending order. Lastly, BMI is a numeric variable that represents the body mass index.\nThe purpose of this exploratory data analysis is to summarize the response variable and three of the predictors (both univariate and bivariate analyses will be performed): BMI, PhysActivity, and Age. These three predictors were chosen as they are commonly cited as some of the most important risk factors of developing type II diabetes. For example, this article says, “Being overweight, being physically inactive, [and] getting older… have long been identified as important risk factors for T2DM” (https://pmc.ncbi.nlm.nih.gov/articles/PMC10518250/). In this project, the ultimate goal of modeling is to find the model that best predicts Diabetes_binary. This will be achieved by analyzing relationships between variables and ultimately minimizing some metric (log loss)."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\n\nlibrary(tidyverse)\n\ndiabetes &lt;- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# check if there are any na values\nhas_na &lt;- any(is.na(diabetes))\n\n# check data to make sure categorical variables are actually categorical\nstr(diabetes |&gt; select(Diabetes_binary, Age, BMI, PhysActivity))\n\ntibble [253,680 × 4] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ Age            : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ BMI            : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ PhysActivity   : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n\n\nThe variables are as expected, with Diabetes_binary, Age, and PhysActivity representing categorical variables, and Age representing a numeric variable. There are no missing values in the data set."
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\n\n# Age, BMI, PhysActivity\nsummary(diabetes |&gt; select(Diabetes_binary, Age, BMI, PhysActivity))\n\n Diabetes_binary       Age              BMI         PhysActivity   \n Min.   :0.0000   Min.   : 1.000   Min.   :12.00   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.: 6.000   1st Qu.:24.00   1st Qu.:1.0000  \n Median :0.0000   Median : 8.000   Median :27.00   Median :1.0000  \n Mean   :0.1393   Mean   : 8.032   Mean   :28.38   Mean   :0.7565  \n 3rd Qu.:0.0000   3rd Qu.:10.000   3rd Qu.:31.00   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :13.000   Max.   :98.00   Max.   :1.0000  \n\n# convert categorical variables to factors\ndiabetes &lt;- diabetes |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"N\", \"Y\")),\n         PhysActivity = factor(PhysActivity, labels = c(\"N\", \"Y\")),\n         Age = factor(Age, ordered = TRUE)\n         )\n\n# create summary stats for BMI and PhysActivity\ndiabetes |&gt; \n  filter(Diabetes_binary %in% c(\"N\", \"Y\")) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    BMI_mean = mean(BMI),\n    BMI_median = median(BMI),\n    BMI_sd = sd(BMI),\n    PhysActivity_prop = mean(PhysActivity == \"Y\")\n  )\n\n# A tibble: 2 × 5\n  Diabetes_binary BMI_mean BMI_median BMI_sd PhysActivity_prop\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n1 N                   27.8         27   6.29             0.777\n2 Y                   31.9         31   7.36             0.631\n\n# create summary stats for Age\ndiabetes |&gt;\n  filter(Diabetes_binary %in% c(\"N\", \"Y\")) |&gt;\n  group_by(Diabetes_binary, Age) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  group_by(Diabetes_binary) |&gt;\n  slice_max(count, n = 1) |&gt;\n  select(Diabetes_binary, most_common_age_group = Age)\n\n# A tibble: 2 × 2\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary most_common_age_group\n  &lt;fct&gt;           &lt;ord&gt;                \n1 N               9                    \n2 Y               10                   \n\n\nPeople with diabetes had higher average BMI (32 vs. 28) and reported less physical activity (63% vs. 78%) than those without diabetes. The most common age group was slightly older in the diabetic group (group 10 vs. group 9). These summary statistics align with expectations: individuals with diabetes had higher BMI, lower physical activity, and tended to be older. This supports the hypothesis that these factors are associated with developing diabetes\n\n# univariate Diabetes_binary plot (bar plot)\nggplot(diabetes, aes(x = Diabetes_binary)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Diabetes Status\",\n       x = \"Diabetes Status\",\n       y = \"Count\")\n\n\n\n# univariate Age plot (bar plot)\nggplot(diabetes, aes(x = Age)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Age\",\n       x = \"Age Group\",\n       y = \"Count\")\n\n\n\n# univariate BMI plot (histogram)\nggplot(diabetes, aes(x = BMI)) +\n  geom_histogram(binwidth = 1) +\n  labs(title = \"Histogram of BMI\",\n       x = \"BMI\",\n       y = \"Count\")\n\n\n\n# univariate PhysActivity plot (bar plot)\nggplot(diabetes, aes(x = PhysActivity)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of PhysActivity\",\n       x = \"PhysActivity\",\n       y = \"Count\")\n\n\n\n\nThe Bar Plot of Diabetes Status shows a significant class imbalance, with many more instances of no diabetes than having diabetes. Even though there’s class imbalance, the minority class (diabetic) still has more than 25,000 instances, which is a large enough sample to provide meaningful insights. The Bar Plot of Age shows a slight class imbalance, but again there are plenty of instances of each class. The Histogram of BMI shows that most instances cluster around about 27, with only a few outliers on the higher end. The Bar Plot of PhysActivity shows a class imbalance, but once again there are enough instances in each class to provide meaningful insights and stable estimates.\n\n# bivariate Age by Diabetes_binary plot (bar plot)\nggplot(diabetes, aes(x = Age, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Distribution of Diabetes Cases across Age Groups\",\n       x = \"Age Group\", y = \"Count\", fill = \"Diabetes Status\") +\n  theme_minimal()\n\n\n\n# bivariate BMI by Diabetes_binary plot (smoothed histogram)\nggplot(diabetes |&gt; filter(BMI &lt; 70), aes(x = BMI)) +\n  geom_density(alpha = 0.5, aes(fill = factor(Diabetes_binary))) +\n  scale_fill_discrete(name = \"Diabetes Status\") +\n  labs(title = \"BMI Distribution by Diabetes Status\", x = \"BMI\", y = \"Density\")\n\n\n\n# bivariate PhysActivity by Diabetes_binary (contingency table)\ntblPhysActivity &lt;- table(diabetes$PhysActivity, diabetes$Diabetes_binary, \n                         dnn = c(\"Physical Activity\", \"Diabetes\"))\ntblPhysActivity\n\n                 Diabetes\nPhysical Activity      N      Y\n                N  48701  13059\n                Y 169633  22287\n\n\nThe Distribution of Diabetes Cases across Age Groups plot shows the general trend that the diabetic group is more associated with older individuals (the diabetic group is centered further to the right than the group without diabetes). The BMI Distribution by Diabetes Status shows the trend that the diabetic group tends to have a higher BMI than the group without diabetes (again, the diabetic group is centered further to the right). The contingency table shows that most individuals reported being physically active, and among them, the rate of diabetes was lower (about 12%) compared to those who were inactive (about 21%). All of this supports the idea that old age, high BMI, and low physical activity are important risk factors for diabetes.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Author: Zachary Rosen\nDate: 7/29/2025"
  },
  {
    "objectID": "Modeling.html#author-and-date",
    "href": "Modeling.html#author-and-date",
    "title": "Modeling",
    "section": "",
    "text": "Author: Zachary Rosen\nDate: 7/29/2025"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "Introduction",
    "text": "Introduction\nIn this project I’ll be working with the Diabetes Health Indicators Dataset which can be found at www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/. Specifically, I will be looking at the diabetes_binary_health_indicators_BRFSS2015.csv data. There are many variables in this dataset, but for the purposes of this project, I will be limiting the scope to the following variables: Diabetes_binary, HighBP, HighChol, Smoker, PhysActivity, Age, and BMI. Each of these variables is categorical except for BMI. Diabetes_binary is the response variable, where 0 represents no diabetes and 1 represents prediabetes or diabetes. The rest of the variables are predictors of Diabetes_binary, where HighBP at level 0 indicates no high blood pressure and 1 indicates high blood pressure, HighChol at level 0 indicates no high cholesterol and 1 indicates high cholesterol, Smoker at level 0 indicates that the person hasn’t smoked at least 100 cigarettes in their life and 1 indicates that they have, and PhysActivity at level 0 indicates no physical activity in the last 30 days (not including a job) and 1 indicates physical activity. Age is a 13 level categorical variable, from younger to older age groups in ascending order. Lastly, BMI is a numeric variable that represents the body mass index.\nThe purpose of this modeling file is to develop and compare predictive models for diabetes status using key predictors. By evaluating logistic regression models, classification trees, and random forests with cross-validated log loss, I aim to identify the best performing model to accurately predict diabetes status. First I will identify the best performing model in each category using cross-validation, and then I will fit a final model in each category using the whole training set. Finally, I will evaluate each of the three “best” models on the testing set and declare a winner."
  },
  {
    "objectID": "Modeling.html#data",
    "href": "Modeling.html#data",
    "title": "Modeling",
    "section": "Data",
    "text": "Data\n\nset.seed(123)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(caret)\nlibrary(ranger)\nlibrary(yardstick)\n\ndiabetes &lt;- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# convert categorical variables to factors\ndiabetes &lt;- diabetes |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"N\", \"Y\")),\n         HighBP = factor(HighBP),\n         HighChol = factor(HighChol),\n         Smoker = factor(Smoker),\n         PhysActivity = factor(PhysActivity),\n         Age = factor(Age, ordered = TRUE)\n         )\n\n# split data into train/test (70/30)\ndata_split &lt;- initial_split(diabetes, prop = 0.7)\ntrain &lt;- training(data_split)\ntest &lt;- testing(data_split)\n\n# 5-fold cv using log loss\ncontrol &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE,\n                            summaryFunction = mnLogLoss)"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nLogistic regression can be used as a classification model when the response variable is binary. It estimates the probability that an observation belongs to a particular class (e.g. diabetic or not). It models the log-odds of the outcome as a linear combination of the predictors. Logistic regression is useful here since I’m working with a binary outcome (Diabetes_binary) and want to understand how different predictors are associated with diabetes risk. It provides interpretable coefficients and works well with log loss as a metric. This is because log loss directly measures the accuracy of predicted probabilities (and logistic regression outputs these probabilities).\n\n# set tuning grid for alpha (0 is ridge regression, 1 is LASSO and 0.5 is elastic net)\n# lambda is the penalty hyperparameter\ngrid_LR &lt;- expand.grid(\n  alpha = c(0, 0.5, 1),\n  lambda = exp(seq(log(0.001), log(10), length.out = 200))\n)\n\n# simple additive model\nLR_1 &lt;- train(Diabetes_binary ~ PhysActivity + Age + BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# similar to LR_1 but with an added interaction term between Age and BMI\nLR_2 &lt;- train(Diabetes_binary ~ PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# similar to LR_2 but with HighBP, HighChol, and Smoker as additional predictors\nLR_3 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                + PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = control,\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = grid_LR\n)\n\n# get training performances for each model\ngetTrainPerf(LR_1)\n\n  TrainlogLoss method\n1    0.3578917 glmnet\n\ngetTrainPerf(LR_2)\n\n  TrainlogLoss method\n1    0.3575414 glmnet\n\ngetTrainPerf(LR_3)\n\n  TrainlogLoss method\n1     0.338274 glmnet\n\n# LR_3 is winner (lowest log loss)\n# fit final model without cv and with best values for hyperparameters\nLR_3_final &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                + PhysActivity + Age*BMI,\n              data = train,\n              metric = \"logLoss\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = trainControl(method = \"none\", classProbs = TRUE),\n              method = \"glmnet\",\n              family = \"binomial\",\n              tuneGrid = LR_3$bestTune\n)\n\n# predict using test data\nLR_3_probs &lt;- predict(LR_3_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_LR_3 &lt;- mn_log_loss(\n  data = bind_cols(test, LR_3_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nLR_3$bestTune\n\n    alpha lambda\n201   0.5  0.001\n\n\nLR_2 has slightly lower log loss than LR_1 and LR_3 has slightly lower log loss than LR_2. This indicates that adding an interaction term between Age and BMI did in fact help predictive performance while training. Similarly, adding HighBP, HighChol, and Smoker as additional predictors also helped predictive performance while training (by quite a lot). The model with the lowest log loss is chosen as the best model (LR_3).\nA classification tree is a decision tree that recursively splits the data based on predictor values to classify observations. At each node, it chooses the best split to maximize class purity (using a metric like Gini impurity). The final model is a tree where each leaf represents a class label. These models are easy to interpret and can handle numeric and categorical predictors. We might use this as they are especially helpful when the relationship between predictors and the outcome is complex."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\n\n# set tuning grid for complexity parameter\ngrid_class_tree &lt;- expand.grid(\n  cp = seq(0.0001, 0.01, length.out = 100)\n)\n\n# use model formula from LR_3 since it performed the best\n# but instead of logistic regression, use classification tree\nclass_tree &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                    data = train,\n                    metric = \"logLoss\",\n                    trControl = control,\n                    method = \"rpart\",\n                    tuneGrid = grid_class_tree\n)\n\n# get training performance\ngetTrainPerf(class_tree)\n\n  TrainlogLoss method\n1    0.3571625  rpart\n\n# fit final model without cv and with best value for hyperparameter\nclass_tree_final &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                    data = train,\n                    metric = \"logLoss\",\n                    trControl = trainControl(method = \"none\", classProbs = TRUE),\n                    method = \"rpart\",\n                    tuneGrid = class_tree$bestTune\n)\n\n# predict using test data\nclass_tree_probs &lt;- predict(class_tree_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_class_tree &lt;- mn_log_loss(\n  data = bind_cols(test, class_tree_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nclass_tree$bestTune\n\n     cp\n1 1e-04\n\n\nThe tuned value of the complexity parameter is 0.0003, which is quite small. This indicates that the model created a larger tree with many splits. Overall the final model is relatively complex, which helps fit the training data well, but increases the risk of overfitting."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "Random Forest",
    "text": "Random Forest\n\n# set tuning grid for mtry (# of randomly selected predictors), splitrule (metric \n# used to determine splitting rule), and min.node.size (minimal node size)\ngrid_rand_forest &lt;- expand.grid(\n  mtry = c(2, 3, 4, 5),\n  splitrule = c(\"gini\"),\n  min.node.size = c(1, 5, 10)\n)\n\n# same model formula as before but using a random forest model\nrand_forest &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                     data = train,\n                     metric = \"logLoss\",\n                     trControl = control,\n                     method = \"ranger\",\n                     tuneGrid = grid_rand_forest\n)\n\nGrowing trees.. Progress: 99%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 65%. Estimated remaining time: 16 seconds.\nGrowing trees.. Progress: 100%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 62%. Estimated remaining time: 18 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 63%. Estimated remaining time: 18 seconds.\nGrowing trees.. Progress: 95%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 82%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 85%. Estimated remaining time: 5 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 90%. Estimated remaining time: 3 seconds.\nGrowing trees.. Progress: 75%. Estimated remaining time: 10 seconds.\nGrowing trees.. Progress: 84%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 76%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 87%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 81%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 98%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 83%. Estimated remaining time: 6 seconds.\nGrowing trees.. Progress: 66%. Estimated remaining time: 15 seconds.\nGrowing trees.. Progress: 80%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 65%. Estimated remaining time: 16 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 0 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 12 seconds.\nGrowing trees.. Progress: 92%. Estimated remaining time: 2 seconds.\nGrowing trees.. Progress: 86%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 88%. Estimated remaining time: 4 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 11 seconds.\nGrowing trees.. Progress: 91%. Estimated remaining time: 3 seconds.\nGrowing trees.. Progress: 69%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 86%. Estimated remaining time: 5 seconds.\n\n# get training performance\ngetTrainPerf(rand_forest)\n\n  TrainlogLoss method\n1    0.3380356 ranger\n\n# fit final model without cv and with best values for hyperparameters\nrand_forest_final &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker \n                      + PhysActivity + Age*BMI,\n                     data = train,\n                     metric = \"logLoss\",\n                     trControl = trainControl(method = \"none\", classProbs = TRUE),\n                     method = \"ranger\",\n                     tuneGrid = rand_forest$bestTune\n)\n\nGrowing trees.. Progress: 87%. Estimated remaining time: 4 seconds.\n\n# predict using test data\nrand_forest_probs &lt;- predict(rand_forest_final, newdata = test, type = \"prob\")\n\n# compute test log loss for final model\nlog_loss_rand_forest &lt;- mn_log_loss(\n  data = bind_cols(test, rand_forest_probs),\n  truth = Diabetes_binary,\n  event_level = \"second\",\n  Y\n)\n\nrand_forest$bestTune\n\n  mtry splitrule min.node.size\n6    3      gini            10\n\n\nThe tuned value of mtry was 3 and min.node.size was 10. This likely helps balance model complexity and overfitting risk.\nA random forest is an ensemble method that builds on many decision trees (e.g. classification trees) on bootstrapped samples of the data and averages their predictions. At each split, only a random subset of predictors is considered, which reduces overfitting. It is often more stable than a traditional classification tree, and is often more accurate and less sensitive to small changes in the data. In our case, it is useful for capturing complex patterns in the data without risking overfitting (like with a traditional classification tree)."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\n\n# make tibble with \"best\" LR, class tree, and random forest models for comparison\ntibble(\n  model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  log_loss = c(log_loss_LR_3$.estimate, log_loss_class_tree$.estimate, \n               log_loss_rand_forest$.estimate)\n) |&gt;\n  mutate(log_loss = format(log_loss, digits = 5))\n\n# A tibble: 3 × 2\n  model               log_loss\n  &lt;chr&gt;               &lt;chr&gt;   \n1 Logistic Regression 0.34001 \n2 Classification Tree 0.35953 \n3 Random Forest       0.34013 \n\n\nThe final “best” model is the logistic regression model, which just barely outperforms the random forest model (log loss of 0.34001 vs. 0.34014). These are both significantly better performing than the classification tree (log loss of 0.35930)."
  }
]